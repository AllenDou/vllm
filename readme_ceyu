#

# requirements
1. L20 * 1
2. vllm latest version

# quantization qwen2.5-{7B, 32B}
python3 awq_quantize_qwen25_32b.py # default q_group_size is 128, 32 is better
python3 awq_quantize_qwen25_7b.py


#Serving
#normal mode
python3 -m vllm.entrypoints.openai.api_server --model /root/Qwen2.5-32B-awq --tensor-parallel-size 1 --served-model-name modelx --disable-log-stats --max-model-len 8192

# speculative decoding mode
python3 -m vllm.entrypoints.openai.api_server --model /root/Qwen2.5-32B-awq --speculative-model /root/Qwen2.5-7B-awq --tensor-parallel-size 1 --served-model-name modelx --disable-log-stats --use-v2-block-manager --num-speculative-tokens 4 --max-model-len 8192


# client
sh openai_completion2.sh


